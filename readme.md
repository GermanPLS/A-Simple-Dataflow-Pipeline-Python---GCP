#  Lab: "A Simple Dataflow Pipeline (Python) 2.5" en Google Cloud Platform (GCP)

Este laboratorio forma parte de la Ruta de aprendizaje del Ingeniero de Datos (Data Engineer Learning Path) en Google Cloud  Skills Boost y tiene como objetivo ense침arte a construir un pipeline de procesamiento por lotes de datos utilizando Apache Beam con Python. 

A lo largo del ejercicio, aprender치s a desarrollar y ejecutar este pipeline de forma local, y luego a desplegarlo en Cloud Dataflow, el servicio gestionado de Google para el procesamiento distribuido de datos.

### url: https://www.cloudskillsboost.google/paths/16/course_templates/53/labs/509055

## Laboratorio

## Tarea 1 - Aseg칰rese de que la API de Dataflow est칠 habilitada correctamente.
Este paso se hace por precauci칩n, para garantizar que el entorno est칠 limpio y listo para usar Dataflow sin conflictos. Es una pr치ctica com칰n en entornos de laboratorio o aprendizaje.

Ejecute el siguiente bloque de c칩digo en Cloud Shell:
```bash
gcloud services disable dataflow.googleapis.com --force
gcloud services enable dataflow.googleapis.com
```


## Tarea 2 - Preparaci칩n.

### - Conexi칩n a la m치quina virtual (VM) mediante SSH.

En este laboratorio, ejecutar치s todo el c칩digo desde una m치quina virtual (VM) configurada con los recursos necesarios para el entrenamiento. Esta VM se encuentra preconfigurada en tu proyecto de GCP y te servir치 como entorno de desarrollo para construir y desplegar el pipeline.

- Desde la consola de GCP, en el men칰 de Navegaci칩n (Icono del men칰 de navegaci칩n), haga clic en **Compute Engine > Instancias de VM** .

- Localice la l칤nea con la instancia llamada `training-vm` .

- En el extremo derecho, en **Conectar** , haga clic en `SSH` para abrir una terminal directamente en la VM desde tu navegador.
![alt text](imagenes/2.png)

游눹 Entorno preconfigurado en la VM training-vm
La m치quina virtual training-vm ya viene con todo lo necesario para realizar el laboratorio, incluyendo:

- Python

- pip

- Apache Beam

- Google Cloud SDK



### - Descargar repositorio de c칩digo.

Para comenzar a trabajar en el laboratorio, primero debes descargar el repositorio de c칩digo que contiene los archivos necesarios para construir el pipeline de datos.

En la terminal SSH de la m치quina virtual de entrenamiento, ejecuta el siguiente comando:
    
    git clone https://github.com/GoogleCloudPlatform/training-data-analyst
    
![alt text](imagenes/2.1.png)

### - Crear un bucket de almacenamiento en Cloud Storage.

Antes de ejecutar cualquier pipeline, aseg칰rate de que NO existe un bucket en Cloud Storage en tu proyecto.
![alt text](imagenes/3.png)

游댌 쯇or qu칠 es necesario crear un bucket?

 Google Cloud Dataflow necesita un bucket de Cloud Storage por varias razones:

- **Staging:** guarda archivos temporales como el c칩digo del pipeline, librer칤as y recursos que se usan durante la ejecuci칩n.

- **Input:** puede leer archivos fuente (como CSV o JSON) almacenados en la nube.

- **Output:** escribe los resultados del pipeline procesado.

- **Separaci칩n entre c칩mputo y almacenamiento:** el procesamiento ocurre en los workers de Dataflow, mientras que los datos residen en el bucket. Esto permite escalar f치cilmente y mantener costos bajos.


Siga estas instrucciones para crear un dep칩sito.

- En la consola, en el men칰 de navegaci칩n , haga clic en **Cloud Storage > Buckets** .

- Haga clic en  **Create bucket**.

- Especifique lo siguiente: `Nombre y Region`  (segun lo que indica el Lab),  y deje las configuraciones restantes como predeterminadas:



- Haga clic en **Crear** .

- haga clic en **Confirmar** .

![alt text](imagenes/3.1.png)

- Desde la terminal SSH de tu VM, `training-vm` ,puedes crear una variable de entorno para referenciar tu bucket f치cilmentey verificar que creo correctamente con el comando `echo`:

    ```bash
    BUCKET="PROJECT ID"
    echo $BUCKET
    ```






## Tarea 3 - Filtrado del Pipeline.


En esta etapa del laboratorio, aprender치s a ejecutar una canalizaci칩n (pipeline) de procesamiento de datos usando Apache Beam y Google Cloud Dataflow. El objetivo es familiarizarte con la estructura de un proyecto de Dataflow, entender c칩mo funciona el filtrado de datos y observar c칩mo se ejecuta la l칩gica desde un archivo Python

- Desde la terminal SSH de tu VM `training-vm` y navega al directorio donde se encuentra el c칩digo del pipeline:

    ```
    cd ~/training-data-analyst/courses/data_analysis/lab2/python
    ```
    ![alt text](imagenes/4.png)
    
- Visualiza el archivo con Nano. 
    ```
    nano grep.py
    ```

    - Presione CTRL+X para salir de Nano.

  ![alt text](imagenes/5.png)


### 쯈u칠 hace el archivo grep.py?

El archivo grep.py implementa un pipeline escrito en Python usando Apache Beam, que se ejecuta en Dataflow, y emula el comportamiento b치sico del comando `grep` en Linux/Unix: **buscar l칤neas de texto que contengan una palabra clave espec칤fica.**

#### 쯇uedes responder estas preguntas sobre el archivo grep.py?


- 쯈u칠 archivos se est치n leyendo?

     Se est치n leyendo archivos de texto con extensi칩n `.java` desde una ubicacion en Cloud Storage. La ruta se define mediante el par치metro **--input** al ejecutar el script.

- 쮺u치l es el t칠rmino de b칰squeda?

   La palabra `import`. El pipeline buscara todas las l칤neas que comiencen con ese t칠rmino.

- 쮸 d칩nde va la salida?

    Las l칤neas filtradas se escriben en archivos ubicados en la carpeta **/tmp/output.**
#### Hay tres transformaciones en la tuber칤a:
- 쯈u칠 hace la primera transformaci칩n?

    Lee todas las l칤neas de texto de los archivos `.java.`

- 쯈u칠 hace la segunda transformaci칩n?

    Filtra las l칤neas, conservando solo aquellas que comienzan con la palabra `import.`

- 쮻e d칩nde proviene su aporte?

    De la salida de la primera transformaci칩n (las l칤neas le칤das del archivo).

- 쯈u칠 hace con esta entrada?

    Eval칰a cada l칤nea y emite solo las que coinciden con el patr칩n buscado.

- 쯈u칠 escribe en su salida?

    Una colecci칩n de l칤neas que comienzan con import.

- 쮸 d칩nde va la salida?
    
    Pasa a la tercera transformaci칩n, que la guarda en archivos de salida.

- 쯈u칠 hace la tercera transformaci칩n?

     Escribe las l칤neas filtradas en archivos dentro de la carpeta /tmp/output.



## Tarea 4 - Ejecutar la canalizaci칩n localmente.

- En la terminal SSH de la m치quina virtual de entrenamiento `training-vm` , ejecute localmente grep.py:

    ```bash
    python3 grep.py
    ```
    Nota : ignore la advertencia si la hay.

El archivo de salida ser치 output.txt. Si el tama침o de salida es suficiente, se dividir치 en partes separadas con nombres como: output-00000-of-00001.

- Localice el archivo correcto examinando la hora del archivo:
    ```
    ls -al /tmp
    ```
    ![alt text](imagenes/6.png)
- Examine los archivos de salida.

- Puede reemplazar "-*" a continuaci칩n con el sufijo apropiado:

    ```
    cat /tmp/output-*
    ```
    ![alt text](imagenes/6.1.png)
  
쮼l resultado parece l칩gico?

## Tarea 5. Ejecutar el pipeline en la nube

- Copia algunos archivos Java a la nube.

     En la terminal SSH de la m치quina virtual de entrenamiento , introduce el siguiente comando:


    ```sh
    # Este comando copia todos los archivos .java que est치n en la ruta local ../javahelp/.../javahelp/* hacia el bucket de Cloud Storage llamado $BUCKET, dentro de la carpeta/prefijo `javahelp`.
    
    gcloud storage cp ../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java gs://$BUCKET/javahelp

    ```

    ![alt text](imagenes/6.2.png)
  
- Usando `nano`, edite la canalizaci칩n de flujo de datos en `grepc.py`:

    ```sh
    # Editamos el archivo grey.py 
    nano grepc.py
    ```
    ![alt text](imagenes/6.3.png)
  
- Reemplace PROJECT, BUCKET y REGION con los valores que se indican a continuaci칩n. Conserve las comillas simples exteriores.

    ```sh
    PROJECT='PROJECT ID'
    ```

    ```sh
    BUCKET='PROJECT ID'
    ```

    ```sh
    REGION='REGION'
    ```
Guarde el archivo y cierre `nano` presionando la tecla **CTRL+X**, luego escriba **Y** y presione **Enter**.

- Env칤e el trabajo de Dataflow a la nube:
    ```sh
    # ejecutamos el script
    python3 grepc.py
    ```
Debido a que se trata de un trabajo tan peque침o, ejecutarlo en la nube tomar치 mucho m치s tiempo que ejecutarlo localmente (alrededor de 7 a 10 minutos).

- Regrese a la pesta침a del navegador para ver la consola.

- En el **men칰 de navegaci칩n** , haga clic en **VER TODOS LOS PRODUCTOS** . En la secci칩n **An치lisis** , haga clic en **Dataflow** y seleccione su trabajo para supervisar el progreso.

- Espere hasta que el **Job status** sea **Exitoso** .
    ![alt text](imagenes/8.png)
    ![alt text](imagenes/9.png)


- En el **men칰 de Navegaci칩n** , haga clic en **Cloud Storage > Buckets**.

- Haga clic en el directorio **javahelp** .

    Este trabajo genera el archivo `output.txt`.

    Si el archivo es lo suficientemente grande, se fragmentar치 en varias partes con nombres como: output-0000x-of-000y. Puede identificar el archivo m치s reciente por su nombre o por el campo **Last modified**.
    ![alt text](imagenes/10.png)

- Haga clic en el archivo para verlo.

    Alternativamente, puede descargar el archivo a trav칠s de la terminal SSH de `training-vm` y verlo:

    ```
    gcloud storage cp gs://$BUCKET/javahelp/output* .
    cat output*
    ```


    #### ARCHIVO OUTPUT

    ```
    import org.apache.beam.sdk.Pipeline;
    import org.apache.beam.sdk.io.TextIO;
    import org.apache.beam.sdk.options.PipelineOptions;
    import org.apache.beam.sdk.options.PipelineOptionsFactory;
    import org.apache.beam.sdk.transforms.DoFn;
    import org.apache.beam.sdk.transforms.ParDo;
    import java.util.ArrayList;
    import java.util.List;
    import org.apache.beam.sdk.Pipeline;
    import org.apache.beam.sdk.io.TextIO;
    import org.apache.beam.sdk.options.Default;
    import org.apache.beam.sdk.options.Description;
    import org.apache.beam.sdk.options.PipelineOptions;
    import org.apache.beam.sdk.options.PipelineOptionsFactory;
    import org.apache.beam.sdk.transforms.DoFn;
    import org.apache.beam.sdk.transforms.ParDo;
    import org.apache.beam.sdk.transforms.Sum;
    import org.apache.beam.sdk.transforms.Top;
    import org.apache.beam.sdk.values.KV;
    import java.util.ArrayList;
    import java.util.Collections;
    import java.util.List;
    import java.util.Map;
    import com.google.api.services.bigquery.model.TableRow;
    import org.apache.beam.sdk.Pipeline;
    import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
    import org.apache.beam.sdk.io.TextIO;
    import org.apache.beam.sdk.options.Default;
    import org.apache.beam.sdk.options.Description;
    import org.apache.beam.sdk.options.PipelineOptions;
    import org.apache.beam.sdk.options.PipelineOptionsFactory;
    import org.apache.beam.sdk.transforms.DoFn;
    import org.apache.beam.sdk.transforms.ParDo;
    import org.apache.beam.sdk.transforms.Sum;
    import org.apache.beam.sdk.transforms.Top;
    import org.apache.beam.sdk.transforms.View;
    import org.apache.beam.sdk.values.KV;
    import org.apache.beam.sdk.values.PCollection;
    import org.apache.beam.sdk.values.PCollectionView;
    import java.time.Instant;
    import java.util.ArrayList;
    import java.util.List;
    import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;
    import org.apache.beam.sdk.Pipeline;
    import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
    import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;
    import org.apache.beam.sdk.options.Default;
    import org.apache.beam.sdk.options.Description;
    import org.apache.beam.sdk.options.PipelineOptionsFactory;
    import org.apache.beam.sdk.transforms.DoFn;
    import org.apache.beam.sdk.transforms.ParDo;
    import org.apache.beam.sdk.transforms.Sum;
    import org.apache.beam.sdk.transforms.windowing.SlidingWindows;
    import org.apache.beam.sdk.transforms.windowing.Window;
    import org.joda.time.Duration;
    import com.google.api.services.bigquery.model.TableFieldSchema;
    import com.google.api.services.bigquery.model.TableRow;
    import com.google.api.services.bigquery.model.TableSchema;

    ```
# Resumen del laboratorio

    
- Accedes a una VM para trabajar en un entorno ya listo para programar con Apache Beam.


- Clon치s el repositorio con el c칩digo de ejemplo.

- Cre치s un bucket en Cloud Storage, que ser치 usado por Dataflow para staging y output.

- Ejecut치s el pipeline localmente con DirectRunner, ideal para pruebas.

- Ejecut치s el pipeline en la nube con DataflowRunner, usando el bucket para staging/output.


